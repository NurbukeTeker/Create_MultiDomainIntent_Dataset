# -*- coding: utf-8 -*-
"""CLINCBertFineTune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i3rth6AGkb5CXVGJD9FqIqDbd1Hf6x34
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow
# %pip install numpy
# %pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %pip install datasets

import pandas as pd
import tensorflow as tf
import torch
import numpy as np
import time
import datetime
import random
import seaborn as sns
import matplotlib.pyplot as plt

import transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup

from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score ,accuracy_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder


device_name = tf.test.gpu_device_name()
device = torch.device("cuda")
print('GPU:', torch.cuda.get_device_name(0))
# set environment as googledrive to folder "resource"

data_path =  "/atis/"


from datasets import load_dataset

dataset_small_train = load_dataset("clinc_oos", "small",split="train")
dataset_small_test = load_dataset("clinc_oos", "small",split="test")
#dataset_imbalanced = load_dataset("clinc_oos", "imbalanced")
#dataset_plus = load_dataset("clinc_oos" ,"plus")

df_train = pd.DataFrame(dataset_small_train)
df_train
df_test = pd.DataFrame(dataset_small_test)
df_test

frames= [ df_train,df_test]
df_all = pd.concat(frames ,ignore_index=True)
df_all

df_all["intent"].value_counts()

df_counts = df_all["intent"].value_counts()
df_counts = df_counts.to_dict()
len(df_counts)
#total 175

df_labels = LabelEncoder().fit_transform(df_all["intent"])
df_labels

df_all["intent"] = df_labels
df_all = df_all.dropna()
df_all

df_all

from sklearn.model_selection import train_test_split

train_acid, test_acid = train_test_split(df_all, test_size=0.2)  #shuffled

train_texts = train_acid["text"].tolist()
train_labels = train_acid["intent"].tolist()

test_texts = test_acid["text"].tolist()
test_labels = test_acid["intent"].tolist()

number_of_categories = len(set(train_labels))
number_of_categories

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
sentences = train_texts
max_len = 20

len(train_texts)

len(test_texts)

input_ids = []
attention_masks = []

for text in train_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                    
                        add_special_tokens = True,
                        max_length = max_len,      
                        pad_to_max_length = True,
                        return_attention_mask = True, 
                        return_tensors = 'pt',
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(train_labels)

print('Original: ', train_texts[0])
print('Token IDs:', input_ids[0])

train_dataset = TensorDataset(input_ids, attention_masks, labels)

batch_size = 32

train_dataloader = DataLoader(
            train_dataset,  
            sampler = RandomSampler(train_dataset), 
            batch_size = batch_size 
        )

number_of_categories = len(set(train_labels))

model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels = number_of_categories, 
    output_attentions = False,
    output_hidden_states = False,
)

model.cuda()

epochs = 20

optimizer = AdamW(model.parameters(),
                  lr = 5e-5,
                  eps = 1e-8 
                )

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

seed_val = 1903

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []
total_t0 = time.time()

for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    t0 = time.time()
    total_train_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        if step % 10 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        model.zero_grad()        
        output = model(b_input_ids, 
                       token_type_ids=None, 
                       attention_mask=b_input_mask, 
                       labels=b_labels)
        loss = output['loss']
        logits = output['logits']
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)            
    training_time = format_time(time.time() - t0)

    print("Average training loss: {0:.2f}".format(avg_train_loss))
    print("Training epoch took: {:}".format(training_time))

    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Training Time': training_time,
        }
    )

print("Training completed in {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

df_stats = pd.DataFrame(data=training_stats)
plt.plot(df_stats['Training Loss'], label="Training")
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.xticks([1, 2, 3, 4])
plt.show()

test_texts = test_texts
test_labels = test_labels

input_ids = []
attention_masks = []

for text in test_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels)

batch_size = 32  

prediction_data = TensorDataset(input_ids, attention_masks, labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print('Prediction started on test data')
model.eval()
predictions , true_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]

f_score = f1_score(test_labels, prediction_scores, average='macro')
precision = precision_score(test_labels, prediction_scores, average='macro')
recall = recall_score(test_labels, prediction_scores, average='macro')
accuracy_score = accuracy_score(test_labels, prediction_scores)

print("Accuracy: ", accuracy_score)
print("F-Score: ", f_score)
print("Recall: ", recall)
print("Precision: ", precision)

train_labels_ = train_labels
train_labels_encodded = LabelEncoder().fit_transform(train_labels_)

report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))

print(report)

from sklearn.metrics import confusion_matrix,multilabel_confusion_matrix,classification_report
cm = multilabel_confusion_matrix(test_labels, prediction_scores)
print( classification_report(test_labels, prediction_scores))

from collections import Counter
Counter(test_labels)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,plot_confusion_matrix
import matplotlib.pyplot as plt


import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_labels, prediction_scores)
f = sns.heatmap(cm, annot=True, fmt='d')

dictionary = dict(zip( train_labels_encodded,train_labels_))
print(dictionary)

report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))
report = report.rename(columns=dictionary)

print(report)

from transformers import AutoModelForSequenceClassification
from transformers import AutoConfig, AutoModel

model.save_pretrained("model_clinc/clinc_model")
#Plot the confusion matrix. Set Normalize = True/False

import sklearn.metrics as skm

cm = skm.multilabel_confusion_matrix(test_labels, prediction_scores)
print(cm)
print( skm.classification_report(test_labels, prediction_scores))

