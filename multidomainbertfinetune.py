# -*- coding: utf-8 -*-
"""MultiDomainBertFinetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zoHuGpHaf7bjVnxX_NQuNiXDJI5Mfefr
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy
# %pip install transformers
# %pip install tensorflow

import pandas as pd
import tensorflow as tf
import torch
import numpy as np
import time
import datetime
import random
import seaborn as sns
import matplotlib.pyplot as plt

import transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder


device_name = tf.test.gpu_device_name()
# if device_name == '/device:GPU:0':
#     device = torch.device("cuda")
#     print('GPU:', torch.cuda.get_device_name(0))
# else:
#     raise SystemError('GPU device not found')

device = torch.device("cuda")
print('GPU:', torch.cuda.get_device_name(0))

# set environment as googledrive to folder "resource"

col_names = ["index","text","intent"]
df_train = pd.read_csv("domain_train.csv",names=col_names, header=None)
df_train

col_names = ["index","text","intent"]
df_train = pd.read_csv("domain_train.csv",names=col_names, header=None)
df_train = df_train.iloc[1:]
df_train["intent"] = LabelEncoder().fit_transform(df_train["intent"])
df_train = df_train.reset_index(drop=True)
del df_train["index"]
df_train

col_names = ["index","text","intent"]
df_test = pd.read_csv("domain_test.csv",names=col_names, header=None)
df_test = df_test.iloc[1:]
df_test["intent"] = LabelEncoder().fit_transform(df_test["intent"])
df_test = df_test.reset_index(drop=True)
del df_test["index"]
df_test

from sklearn.utils import shuffle
df_train = shuffle(df_train)
df_test = shuffle(df_test)

train_texts = df_train["text"]
train_labels= df_train["intent"]

test_texts = df_test["text"]
test_labels= df_test["intent"]

df_test["intent"][1]




from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

labelencoder.fit_transform(df_train["intent"])
labelencoder.fit_transform(df_test["intent"])


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
sentences = train_texts
max_len = 20

input_ids = []
attention_masks = []

for text in train_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True,
                        max_length = max_len,      
                        pad_to_max_length = True,
                        return_attention_mask = True, 
                        return_tensors = 'pt',
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(train_labels)

print('Original: ', train_texts[0])
print('Token IDs:', input_ids[0])

train_dataset = TensorDataset(input_ids, attention_masks, labels)

batch_size = 32

train_dataloader = DataLoader(
            train_dataset,  
            sampler = RandomSampler(train_dataset), 
            batch_size = batch_size 
        )

number_of_categories = len(set(train_labels))

model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels = number_of_categories, 
    output_attentions = False,
    output_hidden_states = False,
)

model.cuda()



import shutil
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard
import os

model_path = '/model'
model_name = "model_weights_{val_loss:.2f}.ckpt"
if os.path.exists(model_path):
  print("Model path exist, clearing all files under model_path")
  shutil.rmtree(model_path)
else:
  print("Creating model_path")
  os.makedirs(model_path)

model_chk_point = ModelCheckpoint(filepath=os.path.join(model_path,model_name),monitor="val_loss",save_best_only=True,save_weights_only=True)

early_stopping = EarlyStopping(monitor="val_loss",min_delta=0.0001,patience=4,verbose=1)

#tensorboard callback
tensorboard_path = "Tensorboard/logs/"
if os.path.exists(tensorboard_path):
  print("Tensorboard path exists, clearing all files under tensorboard_path")
  shutil.rmtree(tensorboard_path)
else:
  print("Creating tensorboard path")
  os.makedirs(tensorboard_path)

tensorboard_cb = TensorBoard(log_dir=tensorboard_path)

#prepare callback list
callback_list = [model_chk_point, tensorboard_cb, early_stopping]

epochs = 20

optimizer = AdamW(model.parameters(),
                  lr = 5e-5,
                  eps = 1e-8 
                )

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

###
seed_val = 1903

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []
total_t0 = time.time()


for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    t0 = time.time()
    total_train_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        if step % 10 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device).long()
        model.zero_grad()        
        output = model(b_input_ids, 
                       token_type_ids=None, 
                       attention_mask=b_input_mask, 
                       labels=b_labels)
        loss = output['loss']
        logits = output['logits']
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)            
    training_time = format_time(time.time() - t0)

    print("Average training loss: {0:.2f}".format(avg_train_loss))
    print("Training epoch took: {:}".format(training_time))

    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Training Time': training_time,
        }
    )

print("Training completed in {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

print(torch.__version__)
# Commented out IPython magic to ensure Python compatibility.
# %ls /content/drive/MyDrive/snips1/Model/

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/MultiDomainDataset/Tensorboard/logs/






df_stats = pd.DataFrame(data=training_stats)
plt.plot(df_stats['Training Loss'], label="Training")
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.xticks([1, 2, 3, 4])
plt.show()

training_stats

test_texts = df_test["text"].tolist()

test_labels = df_test["intent"].tolist()

test_texts = df_test["text"].tolist()
test_labels = df_test["intent"].tolist()

input_ids = []
attention_masks = []

for text in test_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels)

batch_size = 32  

prediction_data = TensorDataset(input_ids, attention_masks, labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print('Prediction started on test data')
model.eval()
predictions , true_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]

f_score = f1_score(test_labels, prediction_scores, average='macro')
precision = precision_score(test_labels, prediction_scores, average='macro')
recall = recall_score(test_labels, prediction_scores, average='macro')
accuracy_score = accuracy_score(test_labels, prediction_scores)


print("Accuracy_score: ", accuracy_score)
print("F-Score: ", f_score)
print("Recall: ", recall)
print("Precision: ", precision)

report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))

print(report)

from sklearn.metrics import confusion_matrix,multilabel_confusion_matrix,classification_report
cm = multilabel_confusion_matrix(test_labels, prediction_scores)
print(cm)
print( classification_report(test_labels, prediction_scores))


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,plot_confusion_matrix
import matplotlib.pyplot as plt


import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_labels, prediction_scores)
f = sns.heatmap(cm, annot=True, fmt='d')


import os
os.makedirs("models")

from transformers import AutoModelForSequenceClassification
from transformers import AutoConfig, AutoModel

model.save_pretrained("models/my_multidomain_model")

#model_ = AutoModel.from_pretrained("models/my_multidomain_model")
pytorch_model = AutoModelForSequenceClassification.from_pretrained("models/my_multidomain_model")

pytorch_model

test_texts_ = df_test["text"].tolist()
test_texts_ = test_texts_[:5]

test_labels_ = df_test["intent"].tolist()
test_labels_ = test_labels_[:5]

input_ids = []
attention_masks = []

for text in test_texts_:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])
        
    
    
test_labels_ = labelencoder.fit_transform( [1, 1, 1 ,1 ,1])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels_.tolist())

batch_size = 32  

prediction_data = TensorDataset(input_ids, attention_masks,labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print('Prediction started on test data')
pytorch_model.eval()
predictions , true_labels = [], []


for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = pytorch_model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to("cpu").numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]



