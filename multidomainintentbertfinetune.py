# -*- coding: utf-8 -*-
"""MultiDomainIntentBertFinetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojE0zs5JSechs9T0WKRh0qKndNDti9_Z
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy
# %pip install transformers
# %pip install tensorflow

import pandas as pd
import tensorflow as tf
import torch
import numpy as np
import time
import datetime
import random
import seaborn as sns
import matplotlib.pyplot as plt

import transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup

from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score, accuracy_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder


device_name = tf.test.gpu_device_name()
device = torch.device("cuda")
print('GPU:', torch.cuda.get_device_name(0))
 

# set environment as googledrive to folder "resource"

data_path =  "/MultiDomainDataset/"


col_names = ["index","text","intent"]
df_train = pd.read_csv("domainIntents_train.csv",names=col_names, header=None)
df_train

col_names = ["index","text","intent"]
df_train = pd.read_csv("domainIntents_train.csv",names=col_names, header=None)
df_train = df_train.iloc[1:]
df_train["intent"] = LabelEncoder().fit_transform(df_train["intent"])
df_train = df_train.reset_index(drop=True)
del df_train["index"]
df_train

col_names = ["index","text","intent"]
df_test = pd.read_csv("domainIntents_test.csv",names=col_names, header=None)
df_test = df_test.iloc[1:]
df_test["intent"] = LabelEncoder().fit_transform(df_test["intent"])
df_test = df_test.reset_index(drop=True)
del df_test["index"]
df_test

from sklearn.utils import shuffle
df_train = shuffle(df_train)
df_test = shuffle(df_test)


from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

labelencoder.fit_transform(df_train["intent"])
labelencoder.fit_transform(df_test["intent"])


train_texts = df_train["text"]
train_labels= df_train["intent"]

test_texts = df_test["text"]
test_labels= df_test["intent"]

from sklearn.preprocessing import LabelEncoder
train_labels = df_train["intent"].tolist()
train_labels_enocded =  LabelEncoder().fit_transform(df_train["intent"])


res = { train_labels_enocded[i]:train_labels[i] for i in range(len(train_labels))}

df_test["intent"]

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
sentences = train_texts
max_len = 30

input_ids = []
attention_masks = []

for text in train_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True,
                        max_length = max_len,      
                        pad_to_max_length = True,
                        return_attention_mask = True, 
                        return_tensors = 'pt',
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(train_labels)

print('Original: ', train_texts[0])
print('Token IDs:', input_ids[0])

train_dataset = TensorDataset(input_ids, attention_masks, labels)

batch_size = 32

train_dataloader = DataLoader(
            train_dataset,  
            sampler = RandomSampler(train_dataset), 
            batch_size = batch_size 
        )

number_of_categories = len(set(train_labels))

model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels = number_of_categories, 
    output_attentions = False,
    output_hidden_states = False,
)

model.cuda()

epochs = 20

optimizer = AdamW(model.parameters(),
                  lr = 5e-5,
                  eps = 1e-8 
                )

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

seed_val = 1903

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []
total_t0 = time.time()

for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    t0 = time.time()
    total_train_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        if step % 10 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device).long()

        model.zero_grad()        
        output = model(b_input_ids, 
                       token_type_ids=None, 
                       attention_mask=b_input_mask, 
                       labels=b_labels)
        loss = output['loss']
        logits = output['logits']
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)            
    training_time = format_time(time.time() - t0)

    print("Average training loss: {0:.2f}".format(avg_train_loss))
    print("Training epoch took: {:}".format(training_time))

    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Training Time': training_time,
        }
    )

print("Training completed in {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

df_stats = pd.DataFrame(data=training_stats)
plt.plot(df_stats['Training Loss'], label="Training")
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.xticks([1, 2, 3, 4])
plt.show()

test_texts = df_test["text"].tolist()
len(test_texts)

test_labels = df_test["intent"].tolist()
len(test_labels)

test_texts = df_test["text"].tolist()
test_labels = df_test["intent"].tolist()

input_ids = []
attention_masks = []

for text in test_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels)

batch_size = 32  

prediction_data = TensorDataset(input_ids, attention_masks, labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print('Prediction started on test data')
model.eval()
predictions , true_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]

f_score = f1_score(test_labels, prediction_scores, average='macro')
precision = precision_score(test_labels, prediction_scores, average='macro')
recall = recall_score(test_labels, prediction_scores, average='macro')
accuracy_score = accuracy_score(test_labels, prediction_scores)

print("Accuracy: ", accuracy_score)
print("F-Score: ", f_score)
print("Recall: ", recall)
print("Precision: ", precision)

report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))

print(report)


import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_labels, prediction_scores)
f = sns.heatmap(cm, annot=True, fmt='d')

from sklearn.metrics import confusion_matrix,multilabel_confusion_matrix,classification_report
cm = multilabel_confusion_matrix(test_labels, prediction_scores)
print(cm)
print( classification_report(test_labels, prediction_scores))


import os
os.makedirs("models")

from transformers import AutoModelForSequenceClassification
from transformers import AutoConfig, AutoModel


model.save_pretrained("models/my_multidomainImtent_model")

#model_ = AutoModel.from_pretrained("models/my_multidomain_model")
pytorch_model = AutoModelForSequenceClassification.from_pretrained("models/my_multidomainImtent_model")

pytorch_model

test_texts_ = df_test["text"].tolist()
test_texts_ = test_texts_[:5]

test_labels_ = df_test["intent"].tolist()
test_labels_ = test_labels_[:5]

input_ids = []
attention_masks = []

for text in test_texts_:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels_)

batch_size = 32  

prediction_data = TensorDataset(input_ids, attention_masks,labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print('Prediction started on test data')
model.eval()
predictions , true_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]

prediction_scores

test_labels_

f_score = f1_score(test_labels_, prediction_scores, average='macro')
precision = precision_score(test_labels_, prediction_scores, average='macro')
recall = recall_score(test_labels_, prediction_scores, average='macro')
accuracy_score = accuracy_score(test_labels, prediction_scores)


print("Accuracy_score: ", accuracy_score)
print("F-Score: ", f_score)
print("Recall: ", recall)
print("Precision: ", precision)

from transformers import AutoModelForSequenceClassification
from transformers import AutoConfig, AutoModel

model.save_pretrained("modelbig/big_model")
